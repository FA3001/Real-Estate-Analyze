# Real Time Processing With Hadoop
## Overview
- Fetch compressed data from a URL.
- Utilize PySpark for data processing, leveraging HDFS for storage and monitoring resources via Apache Hadoop YARN.
- Employ a data generator to simulate streaming data and transmit it to Apache Kafka.
- Implement PySpark (Spark Streaming) to consume and process streaming data from Kafka topics.
- Persist streaming data into Elasticsearch for storage and subsequent visualization using Kibana.
- Store streaming data into MinIO, a cloud-native object storage service.
- Utilize Apache Airflow for orchestrating the entire data pipeline workflow.
## Architecture 
![arch](https://github.com/FA3001/Real_Time_Processing/blob/main/images/arch.png)

